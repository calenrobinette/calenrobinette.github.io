<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>High-Performance Computer Architecture on Food, Coffee, Code</title>
    <link>https://robinette.dev/notes/hpca/</link>
    <description>Recent content in High-Performance Computer Architecture on Food, Coffee, Code</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Mar 2020 20:53:47 -0500</lastBuildDate>
    
	<atom:link href="https://robinette.dev/notes/hpca/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>L11: VLIW</title>
      <link>https://robinette.dev/notes/hpca/l11_-vliw/</link>
      <pubDate>Mon, 02 Mar 2020 20:10:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l11_-vliw/</guid>
      <description>Very Long Instruction Word Superscalar vs VLIW     OOO Superscalar In-Order Superscalar VLIW     IPC &amp;lt;=N &amp;lt;=N 1 Large Inst some work as N &amp;ldquo;normal&amp;rdquo; insts   How do we find indep. inst. Look at &amp;gt;N insts Look at next N insts in program order Just do next large inst   Hardware cost Expensive Less Expensive Even Less Expensive   Help from compiler Compiler can help Needs help Completely depends on compiler    VLIW: The Good and the Bad Good:</description>
    </item>
    
    <item>
      <title>L10: Compiler ILP</title>
      <link>https://robinette.dev/notes/hpca/l10_-compiler-ilp/</link>
      <pubDate>Mon, 02 Mar 2020 20:09:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l10_-compiler-ilp/</guid>
      <description>Compiler ILP Compiler can help avoid limitied ILP because of dependence chains, as well as independent instructions that are far apart because the CPU can only see a &amp;ldquo;limited window&amp;rdquo; of instructions.
Tree Height Reduction Instead of creating cascading dependencies, can group instructions in more efficient ways:
R8 = R2 + R3 + R4 + R5 ADD R8,R2,R3 ADD R8,R8,R4 ADD R8,R8,R5 Instead could be:
ADD R8,R2,R3 ADD R7,R4,R5 ADD R8,R7,R8 This uses associativity, which not instructions have as a property</description>
    </item>
    
    <item>
      <title>L9: Memory Ordering</title>
      <link>https://robinette.dev/notes/hpca/l9_-memory-ordering/</link>
      <pubDate>Mon, 02 Mar 2020 20:08:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l9_-memory-ordering/</guid>
      <description>Memory Ordering How to do LOAD and STORE in an out-of-order processor.
When does memory write happen? For STORE it happens at commit. Has to be done at that point for the same reasons commit happens in program order.
Where does load get data? From a load-store queue. (This seems like ROB for memory insts)
Load-Store Queue (LSQ) Structure like ROB for LOAD and STORE with 4 parameters
   L/S ADDR VAL C     Load or Store Address Value (if any) Complete?</description>
    </item>
    
    <item>
      <title>L8: ReOrder Buffer</title>
      <link>https://robinette.dev/notes/hpca/l8_-reorder-buffer/</link>
      <pubDate>Mon, 02 Mar 2020 20:07:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l8_-reorder-buffer/</guid>
      <description>ReOrder Buffer (ROB) Execeptions in out-of-order execution If something goes wrong during execution has to go to exeception handler. If other instructions are completed in that time, register values may be different then when the instruction originally started. This is also true in mispredicted branches altering register values before the branch was known to be mispredicted. Leading to the other side of the branch using the wrong values. Also, on the mispredicted side there could be an exception caused by an instruction that, we find out later, was never meant to be executed.</description>
    </item>
    
    <item>
      <title>L7: Instruction Scheduling</title>
      <link>https://robinette.dev/notes/hpca/l7_-instruction-scheduling/</link>
      <pubDate>Mon, 02 Mar 2020 20:06:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l7_-instruction-scheduling/</guid>
      <description>Instruction Scheduling How to execute real world programs faster, even with data dependencies. Tomasulo&amp;rsquo;s algorithm was and still is one of the most efficient ways to take the theory of ILP and physically recreate it in hardware.
Improving IPC Using out-of-order processing to obtain &amp;gt;1 IPC
 How to handle control dependencies =&amp;gt; highly accurate branch prediction False data dependencies =&amp;gt; register renaming (RAT) True data dependencies =&amp;gt; out-of-order execution Structural dependencies =&amp;gt; investing in better hardware  We will analyze how to actually impliment register renaming and out-of-order execution</description>
    </item>
    
    <item>
      <title>L6: ILP</title>
      <link>https://robinette.dev/notes/hpca/l6_-ilp/</link>
      <pubDate>Mon, 02 Mar 2020 20:05:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l6_-ilp/</guid>
      <description>Instruction Level Parallelism (ILP) ILP is the theoretical instructions per cycle (IPC) in the best case scenario disregarding almost all limitations.
What happens when we put all instructions in one cycle? We get close to 0 CPI which is good. But any decode/read cycle won&amp;rsquo;t be accessing the correct value if it needed a previous instruction to finish before it. Forwarding only works to send ahead by one cycle (before that cycles operation), so it also doesn&amp;rsquo;t solve the problem where we need results multiple stages earlier in the pipeline.</description>
    </item>
    
    <item>
      <title>L5: Predication</title>
      <link>https://robinette.dev/notes/hpca/l5_-predication/</link>
      <pubDate>Mon, 02 Mar 2020 20:04:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l5_-predication/</guid>
      <description>Predication Using the compiler to avoid hard to predict branches. Branch prediction guesses where the branches are going with no penalty if they are correct. A missed prediction throws away many cycles of work.
Predication is where both sets of instructions are loaded (taken/not taken) and only one is thrown out ensuring waste is only 50% of total work. The advantage of predication is very niche, since if a BPred is fairly accurate it accrues very little penalty on average whereas predication always has a penalty.</description>
    </item>
    
    <item>
      <title>L4: Branches</title>
      <link>https://robinette.dev/notes/hpca/l4_-branches/</link>
      <pubDate>Mon, 02 Mar 2020 20:03:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l4_-branches/</guid>
      <description>Branches Branch in a pipeline example:
BEQ R1,R2,Label means that if R1 and R2 are equal PC &amp;ldquo;jumps&amp;rdquo; to where ever label is. So:
R1 != R2 -&amp;gt; PC++ R1 == R2 -&amp;gt; PC++ + offset to get to Label
When the latter happens all other instructions in the pipeline behind the branch are now wasted cycles and the new useful instructions need to start being fetched.
Branch Prediction Requirements BPred must correctly guess:</description>
    </item>
    
    <item>
      <title>L3: Pipelining</title>
      <link>https://robinette.dev/notes/hpca/l3_-pipelining/</link>
      <pubDate>Mon, 02 Mar 2020 20:02:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l3_-pipelining/</guid>
      <description>Pipelining What is pipelining Exactly like an oil pipe. Long latency for first action but all after 1st are continuous.
For processors this means:
I1, I2,&amp;hellip; are different instructions
   Fetch Read/Decode ALU MEM WRITE     I1       I2 I1      I3 I2 I1     I4 I3 I2 I1    I5 I4 I3 I2 I1    Why wouldn&amp;rsquo;t we get CPI (cycles per instruction) of 1 as expected?</description>
    </item>
    
    <item>
      <title>L2: Metrics and Evaluation</title>
      <link>https://robinette.dev/notes/hpca/l2_-metrics-and-evaluation/</link>
      <pubDate>Mon, 02 Mar 2020 20:01:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l2_-metrics-and-evaluation/</guid>
      <description>Metrics and Evaluation Performance Latency is how long from start -&amp;gt; finish. Throughput is # of processes/second.
Comparing throughput is done via speedup = N:
N = throughput(x)/throughput(y)
whereas latency is:
N = latency(y)/latency(x)
In both instances we get &amp;ldquo;x is N times faster than y&amp;rdquo;
While performance is directly proportional to throughput, it is proportional to 1/latency
 Measuring Performance Performance = 1/execution time
However, execution time depends on user load</description>
    </item>
    
    <item>
      <title>L1: Introduction, Metrics, and Evaluation</title>
      <link>https://robinette.dev/notes/hpca/l1_-introduction-metrics-and-evaluation/</link>
      <pubDate>Mon, 02 Mar 2020 20:00:47 -0500</pubDate>
      
      <guid>https://robinette.dev/notes/hpca/l1_-introduction-metrics-and-evaluation/</guid>
      <description>Introduction, Metrics, and Evaluation What is Computer Architecture Design a computer that is well designed for it&amp;rsquo;s purpose (e.g. the difference between a laptop, desktop, smartphone, server, etc). Laptop and smartphone would be focused on battery life, desktop on power, etc. We need computer architecture to improve performace (speed, battery life, size, weight) and abilities (3D graphics, security, debugging support).
Computer Architecture is the translation of hardware improvements into better performing and more capable software.</description>
    </item>
    
  </channel>
</rss>